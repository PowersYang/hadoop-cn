<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hadoop-AWS module: Integration with Amazon Web Services | Hadoop中文网</title>
    <meta name="description" content="Hadoop官方文档中文社区">
    <link rel="icon" href="/hadoop.jpeg">
    
    <link rel="preload" href="/assets/css/0.styles.299736e7.css" as="style"><link rel="preload" href="/assets/js/app.44e5c4e8.js" as="script"><link rel="preload" href="/assets/js/2.fd33e103.js" as="script"><link rel="preload" href="/assets/js/14.23af424b.js" as="script"><link rel="prefetch" href="/assets/js/10.0dbc29ee.js"><link rel="prefetch" href="/assets/js/100.51e7458e.js"><link rel="prefetch" href="/assets/js/101.cab336f3.js"><link rel="prefetch" href="/assets/js/102.d7e40b1a.js"><link rel="prefetch" href="/assets/js/103.3cd23502.js"><link rel="prefetch" href="/assets/js/104.a263cbd3.js"><link rel="prefetch" href="/assets/js/105.1cf856b7.js"><link rel="prefetch" href="/assets/js/106.1bd66c2f.js"><link rel="prefetch" href="/assets/js/107.7acc52af.js"><link rel="prefetch" href="/assets/js/108.096bf626.js"><link rel="prefetch" href="/assets/js/109.91d5ec0d.js"><link rel="prefetch" href="/assets/js/11.84130fe4.js"><link rel="prefetch" href="/assets/js/110.602bc28b.js"><link rel="prefetch" href="/assets/js/111.531e373d.js"><link rel="prefetch" href="/assets/js/112.dcdb6f77.js"><link rel="prefetch" href="/assets/js/113.7519fa7c.js"><link rel="prefetch" href="/assets/js/114.97ce2cfb.js"><link rel="prefetch" href="/assets/js/115.719819cf.js"><link rel="prefetch" href="/assets/js/116.606be43b.js"><link rel="prefetch" href="/assets/js/117.5f1db751.js"><link rel="prefetch" href="/assets/js/118.4c8b3331.js"><link rel="prefetch" href="/assets/js/119.756df93e.js"><link rel="prefetch" href="/assets/js/12.a85df2ad.js"><link rel="prefetch" href="/assets/js/120.9735cc8b.js"><link rel="prefetch" href="/assets/js/121.de0f88c2.js"><link rel="prefetch" href="/assets/js/122.68c84b42.js"><link rel="prefetch" href="/assets/js/123.26f2d157.js"><link rel="prefetch" href="/assets/js/124.b8bb5eac.js"><link rel="prefetch" href="/assets/js/125.d3748b6f.js"><link rel="prefetch" href="/assets/js/126.74a59909.js"><link rel="prefetch" href="/assets/js/127.e75ea051.js"><link rel="prefetch" href="/assets/js/128.86691d41.js"><link rel="prefetch" href="/assets/js/129.1b73844b.js"><link rel="prefetch" href="/assets/js/13.5cc92b07.js"><link rel="prefetch" href="/assets/js/15.d44d7e96.js"><link rel="prefetch" href="/assets/js/16.9433ad8d.js"><link rel="prefetch" href="/assets/js/17.e88b2671.js"><link rel="prefetch" href="/assets/js/18.621a61a9.js"><link rel="prefetch" href="/assets/js/19.c2269943.js"><link rel="prefetch" href="/assets/js/20.97a13073.js"><link rel="prefetch" href="/assets/js/21.cfddb2e0.js"><link rel="prefetch" href="/assets/js/22.3fc2e173.js"><link rel="prefetch" href="/assets/js/23.b44f9aaa.js"><link rel="prefetch" href="/assets/js/24.5118c903.js"><link rel="prefetch" href="/assets/js/25.d075e5d7.js"><link rel="prefetch" href="/assets/js/26.773f2460.js"><link rel="prefetch" href="/assets/js/27.d0e0d308.js"><link rel="prefetch" href="/assets/js/28.bba4c1c3.js"><link rel="prefetch" href="/assets/js/29.0d309c43.js"><link rel="prefetch" href="/assets/js/3.df8ab865.js"><link rel="prefetch" href="/assets/js/30.dc22d7ad.js"><link rel="prefetch" href="/assets/js/31.f4f5b282.js"><link rel="prefetch" href="/assets/js/32.126829f0.js"><link rel="prefetch" href="/assets/js/33.b7210392.js"><link rel="prefetch" href="/assets/js/34.808d7f11.js"><link rel="prefetch" href="/assets/js/35.92cc0670.js"><link rel="prefetch" href="/assets/js/36.525eca62.js"><link rel="prefetch" href="/assets/js/37.085ca99e.js"><link rel="prefetch" href="/assets/js/38.bd6ba530.js"><link rel="prefetch" href="/assets/js/39.bdcf0246.js"><link rel="prefetch" href="/assets/js/4.03a382ed.js"><link rel="prefetch" href="/assets/js/40.0a881a26.js"><link rel="prefetch" href="/assets/js/41.e2d38f86.js"><link rel="prefetch" href="/assets/js/42.d3be59ab.js"><link rel="prefetch" href="/assets/js/43.861403a7.js"><link rel="prefetch" href="/assets/js/44.64ea788c.js"><link rel="prefetch" href="/assets/js/45.586e01c2.js"><link rel="prefetch" href="/assets/js/46.4e18b7fe.js"><link rel="prefetch" href="/assets/js/47.c1dba008.js"><link rel="prefetch" href="/assets/js/48.da57593b.js"><link rel="prefetch" href="/assets/js/49.8e699822.js"><link rel="prefetch" href="/assets/js/5.21b79be3.js"><link rel="prefetch" href="/assets/js/50.697ac881.js"><link rel="prefetch" href="/assets/js/51.6c32c16e.js"><link rel="prefetch" href="/assets/js/52.d137da31.js"><link rel="prefetch" href="/assets/js/53.22caaa9a.js"><link rel="prefetch" href="/assets/js/54.7f1d1532.js"><link rel="prefetch" href="/assets/js/55.40dbbf5b.js"><link rel="prefetch" href="/assets/js/56.04c24edc.js"><link rel="prefetch" href="/assets/js/57.b15918db.js"><link rel="prefetch" href="/assets/js/58.b3eb784b.js"><link rel="prefetch" href="/assets/js/59.0d462dd8.js"><link rel="prefetch" href="/assets/js/6.a2711138.js"><link rel="prefetch" href="/assets/js/60.ae65015b.js"><link rel="prefetch" href="/assets/js/61.9b15da8f.js"><link rel="prefetch" href="/assets/js/62.ca728b66.js"><link rel="prefetch" href="/assets/js/63.312b3bdd.js"><link rel="prefetch" href="/assets/js/64.56be4886.js"><link rel="prefetch" href="/assets/js/65.dff3c3a3.js"><link rel="prefetch" href="/assets/js/66.95dfff51.js"><link rel="prefetch" href="/assets/js/67.17e7a9be.js"><link rel="prefetch" href="/assets/js/68.e3b5c05a.js"><link rel="prefetch" href="/assets/js/69.9ab8c7f8.js"><link rel="prefetch" href="/assets/js/7.e9473145.js"><link rel="prefetch" href="/assets/js/70.8d72690b.js"><link rel="prefetch" href="/assets/js/71.3c25cf04.js"><link rel="prefetch" href="/assets/js/72.02b02d8d.js"><link rel="prefetch" href="/assets/js/73.2a8d2681.js"><link rel="prefetch" href="/assets/js/74.c577aba5.js"><link rel="prefetch" href="/assets/js/75.7d2617b0.js"><link rel="prefetch" href="/assets/js/76.97ad371e.js"><link rel="prefetch" href="/assets/js/77.4495b015.js"><link rel="prefetch" href="/assets/js/78.c7510b35.js"><link rel="prefetch" href="/assets/js/79.dc2b4ca2.js"><link rel="prefetch" href="/assets/js/8.fc1e2b26.js"><link rel="prefetch" href="/assets/js/80.454a982e.js"><link rel="prefetch" href="/assets/js/81.eba66eb5.js"><link rel="prefetch" href="/assets/js/82.1f34b986.js"><link rel="prefetch" href="/assets/js/83.a15e7917.js"><link rel="prefetch" href="/assets/js/84.212c8551.js"><link rel="prefetch" href="/assets/js/85.c077f397.js"><link rel="prefetch" href="/assets/js/86.70192242.js"><link rel="prefetch" href="/assets/js/87.e53f7254.js"><link rel="prefetch" href="/assets/js/88.d2f28869.js"><link rel="prefetch" href="/assets/js/89.9b4c13ce.js"><link rel="prefetch" href="/assets/js/9.a7fc3cc9.js"><link rel="prefetch" href="/assets/js/90.af82565a.js"><link rel="prefetch" href="/assets/js/91.153dc863.js"><link rel="prefetch" href="/assets/js/92.ea32e690.js"><link rel="prefetch" href="/assets/js/93.4c656124.js"><link rel="prefetch" href="/assets/js/94.b2283b24.js"><link rel="prefetch" href="/assets/js/95.4ad17bba.js"><link rel="prefetch" href="/assets/js/96.4c0c4821.js"><link rel="prefetch" href="/assets/js/97.14f25834.js"><link rel="prefetch" href="/assets/js/98.92c9fd2d.js"><link rel="prefetch" href="/assets/js/99.ef340791.js">
    <link rel="stylesheet" href="/assets/css/0.styles.299736e7.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Hadoop中文网</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener noreferrer" class="nav-link external">
  下载安装
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/docs/" class="nav-link router-link-active">参考文档</a></div><div class="nav-item"><a href="/docs/awesome/" class="nav-link">资源教程</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="语言支持" class="dropdown-title"><span class="title">语言支持</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/language/ch.html" class="nav-link">简体中文</a></li><li class="dropdown-item"><!----> <a href="/language/en.html" class="nav-link">English</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener noreferrer" class="nav-link external">
  下载安装
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/docs/" class="nav-link router-link-active">参考文档</a></div><div class="nav-item"><a href="/docs/awesome/" class="nav-link">资源教程</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="语言支持" class="dropdown-title"><span class="title">语言支持</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/language/ch.html" class="nav-link">简体中文</a></li><li class="dropdown-item"><!----> <a href="/language/en.html" class="nav-link">English</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Hadoop-AWS module: Integration with Amazon Web Services</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/hadoop-aws/tools/hadoop-aws/#overview" class="sidebar-link">Overview</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/hadoop-aws/tools/hadoop-aws/#introducing-the-hadoop-s3a-client" class="sidebar-link">Introducing the Hadoop S3A client.</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#other-s3-connectors" class="sidebar-link">Other S3 Connectors</a></li></ul></li><li><a href="/docs/hadoop-aws/tools/hadoop-aws/#getting-started" class="sidebar-link">Getting Started</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#buffering-upload-data-on-disk-fs-s3a-fast-upload-buffer-disk" class="sidebar-link">Buffering upload data on disk fs.s3a.fast.upload.buffer=disk</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#buffering-upload-data-in-bytebuffers-fs-s3a-fast-upload-buffer-bytebuffer" class="sidebar-link">Buffering upload data in ByteBuffers: fs.s3a.fast.upload.buffer=bytebuffer</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#buffering-upload-data-in-byte-arrays-fs-s3a-fast-upload-buffer-array" class="sidebar-link">Buffering upload data in byte arrays: fs.s3a.fast.upload.buffer=array</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#upload-thread-tuning" class="sidebar-link">Upload Thread Tuning</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#cleaning-up-after-partial-upload-failures" class="sidebar-link">Cleaning up after partial Upload Failures</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#s3a-“fadvise”-input-policy-support" class="sidebar-link">S3A “fadvise” input policy support</a></li></ul></li><li><a href="/docs/hadoop-aws/tools/hadoop-aws/#metrics" class="sidebar-link">Metrics</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/hadoop-aws/tools/hadoop-aws/#other-topics" class="sidebar-link">Other Topics</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/hadoop-aws/tools/hadoop-aws/#copying-data-with-distcp" class="sidebar-link">Copying Data with distcp</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="hadoop-aws-module-integration-with-amazon-web-services"><a href="#hadoop-aws-module-integration-with-amazon-web-services" class="header-anchor">#</a> Hadoop-AWS module: Integration with Amazon Web Services</h1> <h2 id="overview"><a href="#overview" class="header-anchor">#</a> Overview</h2> <p>Apache Hadoop’s hadoop-aws module provides support for AWS integration. applications to easily use this support.</p> <p>To include the S3A client in Apache Hadoop’s default classpath:</p> <ol><li><p>Make sure thatHADOOP_OPTIONAL_TOOLS in hadoop-env.sh includes hadoop-aws in its list of optional modules to add in the classpath.</p></li> <li><p>For client side interaction, you can declare that relevant JARs must be loaded in your ~/.hadooprc file:</p> <p>hadoop_add_to_classpath_tools hadoop-aws</p></li></ol> <p>The settings in this file does not propagate to deployed applications, but it will work for local clients such as the hadoop fs command.</p> <h2 id="introducing-the-hadoop-s3a-client"><a href="#introducing-the-hadoop-s3a-client" class="header-anchor">#</a> Introducing the Hadoop S3A client.</h2> <p>Hadoop’s “S3A” client offers high-performance IO against Amazon S3 object store and compatible implementations.</p> <h3 id="other-s3-connectors"><a href="#other-s3-connectors" class="header-anchor">#</a> Other S3 Connectors</h3> <p>There other Hadoop connectors to S3. Only S3A is actively maintained by the Hadoop project itself.</p> <ol><li>Apache’s Hadoop’s original s3:// client. This is no longer included in Hadoop.</li> <li>Amazon EMR’s s3:// client. This is from the Amazon EMR team, who actively maintain it.</li> <li>Apache’s Hadoop’s <a href="/docs/hadoop-aws/tools/hadoop-aws/s3n.html">s3n: filesystem client</a>. This connector is no longer available: users must migrate to the newer s3a: client.</li></ol> <h2 id="getting-started"><a href="#getting-started" class="header-anchor">#</a> Getting Started</h2> <p>S3A depends upon two JARs, alongside hadoop-common and its dependencies.</p> <h3 id="buffering-upload-data-on-disk-fs-s3a-fast-upload-buffer-disk"><a href="#buffering-upload-data-on-disk-fs-s3a-fast-upload-buffer-disk" class="header-anchor">#</a> Buffering upload data on disk fs.s3a.fast.upload.buffer=disk</h3> <p>When fs.s3a.fast.upload.buffer is set to disk, all data is buffered to local hard disks prior to upload. This minimizes the amount of memory consumed, and so eliminates heap size as the limiting factor in queued uploads —exactly as the original “direct to disk” buffering.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;disk&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
  &lt;description&gt;Comma separated list of directories that will be used to buffer file
    uploads to.&lt;/description&gt;
&lt;/property&gt;
</code></pre> <p>This is the default buffer mechanism. The amount of data which can be buffered is limited by the amount of available disk space.</p> <h3 id="buffering-upload-data-in-bytebuffers-fs-s3a-fast-upload-buffer-bytebuffer"><a href="#buffering-upload-data-in-bytebuffers-fs-s3a-fast-upload-buffer-bytebuffer" class="header-anchor">#</a> Buffering upload data in ByteBuffers: fs.s3a.fast.upload.buffer=bytebuffer</h3> <p>When fs.s3a.fast.upload.buffer is set to bytebuffer, all data is buffered in “Direct” ByteBuffers prior to upload. This may be faster than buffering to disk, and, if disk space is small (for example, tiny EC2 VMs), there may not be much disk space to buffer with.</p> <p>The ByteBuffers are created in the memory of the JVM, but not in the Java Heap itself. The amount of data which can be buffered is limited by the Java runtime, the operating system, and, for YARN applications, the amount of memory requested for each container.</p> <p>The slower the upload bandwidth to S3, the greater the risk of running out of memory —and so the more care is needed in tuning the upload settings.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;bytebuffer&lt;/value&gt;
&lt;/property&gt;
</code></pre> <h3 id="buffering-upload-data-in-byte-arrays-fs-s3a-fast-upload-buffer-array"><a href="#buffering-upload-data-in-byte-arrays-fs-s3a-fast-upload-buffer-array" class="header-anchor">#</a> Buffering upload data in byte arrays: fs.s3a.fast.upload.buffer=array</h3> <p>When fs.s3a.fast.upload.buffer is set to array, all data is buffered in byte arrays in the JVM’s heap prior to upload. This may be faster than buffering to disk.</p> <p>The amount of data which can be buffered is limited by the available size of the JVM heap heap. The slower the write bandwidth to S3, the greater the risk of heap overflows. This risk can be mitigated by tuning the upload settings.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.buffer&lt;/name&gt;
  &lt;value&gt;array&lt;/value&gt;
&lt;/property&gt;
</code></pre> <h3 id="upload-thread-tuning"><a href="#upload-thread-tuning" class="header-anchor">#</a> Upload Thread Tuning</h3> <p>Both the Array and Byte buffer buffer mechanisms can consume very large amounts of memory, on-heap or off-heap respectively. The disk buffer mechanism does not use much memory up, but will consume hard disk capacity.</p> <p>If there are many output streams being written to in a single process, the amount of memory or disk used is the multiple of all stream’s active memory/disk use.</p> <p>Careful tuning may be needed to reduce the risk of running out memory, especially if the data is buffered in memory.</p> <p>There are a number parameters which can be tuned:</p> <ol><li><p>The total number of threads available in the filesystem for data uploads or any other queued filesystem operation. This is set in fs.s3a.threads.max</p></li> <li><p>The number of operations which can be queued for execution:, awaiting a thread: fs.s3a.max.total.tasks</p></li> <li><p>The number of blocks which a single output stream can have active, that is: being uploaded by a thread, or queued in the filesystem thread queue: fs.s3a.fast.upload.active.blocks</p></li> <li><p>How long an idle thread can stay in the thread pool before it is retired: fs.s3a.threads.keepalivetime</p></li></ol> <p>When the maximum allowed number of active blocks of a single stream is reached, no more blocks can be uploaded from that stream until one or more of those active blocks’ uploads completes. That is: a write() call which would trigger an upload of a now full datablock, will instead block until there is capacity in the queue.</p> <p>How does that come together?</p> <ul><li><p>As the pool of threads set in fs.s3a.threads.max is shared (and intended to be used across all threads), a larger number here can allow for more parallel operations. However, as uploads require network bandwidth, adding more threads does not guarantee speedup.</p></li> <li><p>The extra queue of tasks for the thread pool (fs.s3a.max.total.tasks) covers all ongoing background S3A operations (future plans include: parallelized rename operations, asynchronous directory operations).</p></li> <li><p>When using memory buffering, a small value of fs.s3a.fast.upload.active.blocks limits the amount of memory which can be consumed per stream.</p></li> <li><p>When using disk buffering a larger value of fs.s3a.fast.upload.active.blocks does not consume much memory. But it may result in a large number of blocks to compete with other filesystem operations.</p></li></ul> <p>We recommend a low value of fs.s3a.fast.upload.active.blocks; enough to start background upload without overloading other parts of the system, then experiment to see if higher values deliver more throughput —especially from VMs running on EC2.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload.active.blocks&lt;/name&gt;
  &lt;value&gt;4&lt;/value&gt;
  &lt;description&gt;
    Maximum Number of blocks a single output stream can have
    active (uploading, or queued to the central FileSystem
    instance's pool of queued operations.

    This stops a single stream overloading the shared thread pool.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt;The total number of threads available in the filesystem for data
    uploads *or any other queued filesystem operation*.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.max.total.tasks&lt;/name&gt;
  &lt;value&gt;5&lt;/value&gt;
  &lt;description&gt;The number of operations which can be queued for execution&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.keepalivetime&lt;/name&gt;
  &lt;value&gt;60&lt;/value&gt;
  &lt;description&gt;Number of seconds a thread can be idle before being
    terminated.&lt;/description&gt;
&lt;/property&gt;
</code></pre> <h3 id="cleaning-up-after-partial-upload-failures"><a href="#cleaning-up-after-partial-upload-failures" class="header-anchor">#</a> Cleaning up after partial Upload Failures</h3> <p>There are two mechanisms for cleaning up after leftover multipart uploads: - Hadoop s3guard CLI commands for listing and deleting uploads by their age. Documented in the <a href="/docs/hadoop-aws/tools/hadoop-aws/s3guard.html">S3Guard</a> section. - The configuration parameter fs.s3a.multipart.purge, covered below.</p> <p>If a large stream write operation is interrupted, there may be intermediate partitions uploaded to S3 —data which will be billed for.</p> <p>These charges can be reduced by enabling fs.s3a.multipart.purge, and setting a purge time in seconds, such as 86400 seconds —24 hours. When an S3A FileSystem instance is instantiated with the purge time greater than zero, it will, on startup, delete all outstanding partition requests older than this time.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;True if you want to purge existing multipart uploads that may not have been
     completed/aborted correctly&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge.age&lt;/name&gt;
  &lt;value&gt;86400&lt;/value&gt;
  &lt;description&gt;Minimum age in seconds of multipart uploads to purge&lt;/description&gt;
&lt;/property&gt;
</code></pre> <p>If an S3A client is instantiated with fs.s3a.multipart.purge=true, it will delete all out of date uploads in the entire bucket. That is: it will affect all multipart uploads to that bucket, from all applications.</p> <p>Leaving fs.s3a.multipart.purge to its default, false, means that the client will not make any attempt to reset or change the partition rate.</p> <p>The best practise for using this option is to disable multipart purges in normal use of S3A, enabling only in manual/scheduled housekeeping operations.</p> <h3 id="s3a-“fadvise”-input-policy-support"><a href="#s3a-“fadvise”-input-policy-support" class="header-anchor">#</a> S3A “fadvise” input policy support</h3> <p>The S3A Filesystem client supports the notion of input policies, similar to that of the Posix fadvise() API call. This tunes the behavior of the S3A client to optimise HTTP GET requests for the different use cases.</p> <p>See <a href="/docs/hadoop-aws/tools/hadoop-aws/performance.html#fadvise">Improving data input performance through fadvise</a> for the details.</p> <h2 id="metrics"><a href="#metrics" class="header-anchor">#</a> Metrics</h2> <p>S3A metrics can be monitored through Hadoop’s metrics2 framework. S3A creates its own metrics system called s3a-file-system, and each instance of the client will create its own metrics source, named with a JVM-unique numerical ID.</p> <p>As a simple example, the following can be added to hadoop-metrics2.properties to write all S3A metrics to a log file every 10 seconds:</p> <pre><code>s3a-file-system.sink.my-metrics-config.class=org.apache.hadoop.metrics2.sink.FileSink
s3a-file-system.sink.my-metrics-config.filename=/var/log/hadoop-yarn/s3a-metrics.out
*.period=10
</code></pre> <p>Lines in that file will be structured like the following:</p> <pre><code>1511208770680 s3aFileSystem.s3aFileSystem: Context=s3aFileSystem, s3aFileSystemId=892b02bb-7b30-4ffe-80ca-3a9935e1d96e, bucket=bucket,
Hostname=hostname-1.hadoop.apache.com, files_created=1, files_copied=2, files_copied_bytes=10000, files_deleted=5, fake_directories_deleted=3,
directories_created=3, directories_deleted=0, ignored_errors=0, op_copy_from_local_file=0, op_exists=0, op_get_file_status=15, op_glob_status=0,
op_is_directory=0, op_is_file=0, op_list_files=0, op_list_located_status=0, op_list_status=3, op_mkdirs=1, op_rename=2, object_copy_requests=0,
object_delete_requests=6, object_list_requests=23, object_continue_list_requests=0, object_metadata_requests=46, object_multipart_aborted=0,
object_put_bytes=0, object_put_requests=4, object_put_requests_completed=4, stream_write_failures=0, stream_write_block_uploads=0,
stream_write_block_uploads_committed=0, stream_write_block_uploads_aborted=0, stream_write_total_time=0, stream_write_total_data=0,
s3guard_metadatastore_put_path_request=10, s3guard_metadatastore_initialization=0, object_put_requests_active=0, object_put_bytes_pending=0,
stream_write_block_uploads_active=0, stream_write_block_uploads_pending=0, stream_write_block_uploads_data_pending=0,
S3guard_metadatastore_put_path_latencyNumOps=0, S3guard_metadatastore_put_path_latency50thPercentileLatency=0,
S3guard_metadatastore_put_path_latency75thPercentileLatency=0, S3guard_metadatastore_put_path_latency90thPercentileLatency=0,
S3guard_metadatastore_put_path_latency95thPercentileLatency=0, S3guard_metadatastore_put_path_latency99thPercentileLatency=0
</code></pre> <p>Depending on other configuration, metrics from other systems, contexts, etc. may also get recorded, for example the following:</p> <pre><code>1511208770680 metricssystem.MetricsSystem: Context=metricssystem, Hostname=s3a-metrics-4.gce.cloudera.com, NumActiveSources=1, NumAllSources=1,
NumActiveSinks=1, NumAllSinks=0, Sink_fileNumOps=2, Sink_fileAvgTime=1.0, Sink_fileDropped=0, Sink_fileQsize=0, SnapshotNumOps=5,
SnapshotAvgTime=0.0, PublishNumOps=2, PublishAvgTime=0.0, DroppedPubAll=0
</code></pre> <p>Note that low-level metrics from the AWS SDK itself are not currently included in these metrics.</p> <h2 id="other-topics"><a href="#other-topics" class="header-anchor">#</a> Other Topics</h2> <h3 id="copying-data-with-distcp"><a href="#copying-data-with-distcp" class="header-anchor">#</a> Copying Data with distcp</h3> <p>Hadoop’s distcp tool is often used to copy data between a Hadoop cluster and Amazon S3. See <a href="https://hortonworks.github.io/hdp-aws/s3-copy-data/index.html" target="_blank" rel="noopener noreferrer">Copying Data Between a Cluster and Amazon S3<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for details on S3 copying specifically.</p> <p>The distcp update command tries to do incremental updates of data. It is straightforward to verify when files do not match when they are of different length, but not when they are the same size.</p> <p>Distcp addresses this by comparing file checksums on the source and destination filesystems, which it tries to do even if the filesystems have incompatible checksum algorithms.</p> <p>The S3A connector can provide the HTTP etag header to the caller as the checksum of the uploaded file. Doing so will break distcp operations between hdfs and s3a.</p> <p>For this reason, the etag-as-checksum feature is disabled by default.</p> <pre><code>&lt;property&gt;
  &lt;name&gt;fs.s3a.etag.checksum.enabled&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;
    Should calls to getFileChecksum() return the etag value of the remote
    object.
    WARNING: if enabled, distcp operations between HDFS and S3 will fail unless
    -skipcrccheck is set.
  &lt;/description&gt;
&lt;/property&gt;
</code></pre> <p>If enabled, distcp between two S3 buckets can use the checksum to compare objects. Their checksums should be identical if they were either each uploaded as a single file PUT, or, if in a multipart PUT, in blocks of the same size, as configured by the value fs.s3a.multipart.size.</p> <p>To disable checksum verification in distcp, use the -skipcrccheck option:</p> <pre><code>hadoop distcp -update -skipcrccheck -numListstatusThreads 40 /user/alice/datasets s3a://alice-backup/datasets
</code></pre></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.44e5c4e8.js" defer></script><script src="/assets/js/2.fd33e103.js" defer></script><script src="/assets/js/14.23af424b.js" defer></script>
  </body>
</html>
