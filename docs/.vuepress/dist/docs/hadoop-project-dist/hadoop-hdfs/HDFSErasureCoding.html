<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>HDFS Erasure Coding | Hadoop中文网</title>
    <meta name="description" content="Hadoop官方文档中文社区">
    <link rel="icon" href="/hadoop.jpeg">
    
    <link rel="preload" href="/assets/css/0.styles.fefea12e.css" as="style"><link rel="preload" href="/assets/js/app.03e63b66.js" as="script"><link rel="preload" href="/assets/js/2.fd33e103.js" as="script"><link rel="preload" href="/assets/js/62.ca728b66.js" as="script"><link rel="prefetch" href="/assets/js/10.0dbc29ee.js"><link rel="prefetch" href="/assets/js/100.51e7458e.js"><link rel="prefetch" href="/assets/js/101.cab336f3.js"><link rel="prefetch" href="/assets/js/102.d7e40b1a.js"><link rel="prefetch" href="/assets/js/103.3cd23502.js"><link rel="prefetch" href="/assets/js/104.a263cbd3.js"><link rel="prefetch" href="/assets/js/105.1cf856b7.js"><link rel="prefetch" href="/assets/js/106.1bd66c2f.js"><link rel="prefetch" href="/assets/js/107.7acc52af.js"><link rel="prefetch" href="/assets/js/108.096bf626.js"><link rel="prefetch" href="/assets/js/109.91d5ec0d.js"><link rel="prefetch" href="/assets/js/11.84130fe4.js"><link rel="prefetch" href="/assets/js/110.602bc28b.js"><link rel="prefetch" href="/assets/js/111.490d917f.js"><link rel="prefetch" href="/assets/js/112.dcdb6f77.js"><link rel="prefetch" href="/assets/js/113.0a64a78e.js"><link rel="prefetch" href="/assets/js/114.97ce2cfb.js"><link rel="prefetch" href="/assets/js/115.719819cf.js"><link rel="prefetch" href="/assets/js/116.606be43b.js"><link rel="prefetch" href="/assets/js/117.5f1db751.js"><link rel="prefetch" href="/assets/js/118.4c8b3331.js"><link rel="prefetch" href="/assets/js/119.756df93e.js"><link rel="prefetch" href="/assets/js/12.a85df2ad.js"><link rel="prefetch" href="/assets/js/120.21755a3d.js"><link rel="prefetch" href="/assets/js/121.86335dec.js"><link rel="prefetch" href="/assets/js/122.68c84b42.js"><link rel="prefetch" href="/assets/js/123.26f2d157.js"><link rel="prefetch" href="/assets/js/124.b8bb5eac.js"><link rel="prefetch" href="/assets/js/125.d3748b6f.js"><link rel="prefetch" href="/assets/js/126.74a59909.js"><link rel="prefetch" href="/assets/js/127.e75ea051.js"><link rel="prefetch" href="/assets/js/128.86691d41.js"><link rel="prefetch" href="/assets/js/129.1b73844b.js"><link rel="prefetch" href="/assets/js/13.5cc92b07.js"><link rel="prefetch" href="/assets/js/14.23af424b.js"><link rel="prefetch" href="/assets/js/15.d44d7e96.js"><link rel="prefetch" href="/assets/js/16.9433ad8d.js"><link rel="prefetch" href="/assets/js/17.e88b2671.js"><link rel="prefetch" href="/assets/js/18.621a61a9.js"><link rel="prefetch" href="/assets/js/19.c2269943.js"><link rel="prefetch" href="/assets/js/20.97a13073.js"><link rel="prefetch" href="/assets/js/21.cfddb2e0.js"><link rel="prefetch" href="/assets/js/22.3fc2e173.js"><link rel="prefetch" href="/assets/js/23.b44f9aaa.js"><link rel="prefetch" href="/assets/js/24.5118c903.js"><link rel="prefetch" href="/assets/js/25.d075e5d7.js"><link rel="prefetch" href="/assets/js/26.773f2460.js"><link rel="prefetch" href="/assets/js/27.d0e0d308.js"><link rel="prefetch" href="/assets/js/28.bba4c1c3.js"><link rel="prefetch" href="/assets/js/29.0d309c43.js"><link rel="prefetch" href="/assets/js/3.df8ab865.js"><link rel="prefetch" href="/assets/js/30.238bac85.js"><link rel="prefetch" href="/assets/js/31.abd71d38.js"><link rel="prefetch" href="/assets/js/32.126829f0.js"><link rel="prefetch" href="/assets/js/33.b7210392.js"><link rel="prefetch" href="/assets/js/34.808d7f11.js"><link rel="prefetch" href="/assets/js/35.92cc0670.js"><link rel="prefetch" href="/assets/js/36.525eca62.js"><link rel="prefetch" href="/assets/js/37.085ca99e.js"><link rel="prefetch" href="/assets/js/38.bd6ba530.js"><link rel="prefetch" href="/assets/js/39.bdcf0246.js"><link rel="prefetch" href="/assets/js/4.03a382ed.js"><link rel="prefetch" href="/assets/js/40.5e383ac6.js"><link rel="prefetch" href="/assets/js/41.fb3d104e.js"><link rel="prefetch" href="/assets/js/42.d3be59ab.js"><link rel="prefetch" href="/assets/js/43.861403a7.js"><link rel="prefetch" href="/assets/js/44.64ea788c.js"><link rel="prefetch" href="/assets/js/45.586e01c2.js"><link rel="prefetch" href="/assets/js/46.4e18b7fe.js"><link rel="prefetch" href="/assets/js/47.c1dba008.js"><link rel="prefetch" href="/assets/js/48.da57593b.js"><link rel="prefetch" href="/assets/js/49.8e699822.js"><link rel="prefetch" href="/assets/js/5.21b79be3.js"><link rel="prefetch" href="/assets/js/50.697ac881.js"><link rel="prefetch" href="/assets/js/51.6c32c16e.js"><link rel="prefetch" href="/assets/js/52.d137da31.js"><link rel="prefetch" href="/assets/js/53.22caaa9a.js"><link rel="prefetch" href="/assets/js/54.7f1d1532.js"><link rel="prefetch" href="/assets/js/55.40dbbf5b.js"><link rel="prefetch" href="/assets/js/56.04c24edc.js"><link rel="prefetch" href="/assets/js/57.b15918db.js"><link rel="prefetch" href="/assets/js/58.b3eb784b.js"><link rel="prefetch" href="/assets/js/59.0d462dd8.js"><link rel="prefetch" href="/assets/js/6.64edd77a.js"><link rel="prefetch" href="/assets/js/60.ae65015b.js"><link rel="prefetch" href="/assets/js/61.9b15da8f.js"><link rel="prefetch" href="/assets/js/63.312b3bdd.js"><link rel="prefetch" href="/assets/js/64.56be4886.js"><link rel="prefetch" href="/assets/js/65.dff3c3a3.js"><link rel="prefetch" href="/assets/js/66.95dfff51.js"><link rel="prefetch" href="/assets/js/67.17e7a9be.js"><link rel="prefetch" href="/assets/js/68.e3b5c05a.js"><link rel="prefetch" href="/assets/js/69.9ab8c7f8.js"><link rel="prefetch" href="/assets/js/7.4a3ae97d.js"><link rel="prefetch" href="/assets/js/70.8d72690b.js"><link rel="prefetch" href="/assets/js/71.3c25cf04.js"><link rel="prefetch" href="/assets/js/72.02b02d8d.js"><link rel="prefetch" href="/assets/js/73.2a8d2681.js"><link rel="prefetch" href="/assets/js/74.c577aba5.js"><link rel="prefetch" href="/assets/js/75.7d2617b0.js"><link rel="prefetch" href="/assets/js/76.97ad371e.js"><link rel="prefetch" href="/assets/js/77.4495b015.js"><link rel="prefetch" href="/assets/js/78.c7510b35.js"><link rel="prefetch" href="/assets/js/79.dc2b4ca2.js"><link rel="prefetch" href="/assets/js/8.fc1e2b26.js"><link rel="prefetch" href="/assets/js/80.454a982e.js"><link rel="prefetch" href="/assets/js/81.eba66eb5.js"><link rel="prefetch" href="/assets/js/82.1f34b986.js"><link rel="prefetch" href="/assets/js/83.a15e7917.js"><link rel="prefetch" href="/assets/js/84.212c8551.js"><link rel="prefetch" href="/assets/js/85.c077f397.js"><link rel="prefetch" href="/assets/js/86.70192242.js"><link rel="prefetch" href="/assets/js/87.e53f7254.js"><link rel="prefetch" href="/assets/js/88.d2f28869.js"><link rel="prefetch" href="/assets/js/89.9b4c13ce.js"><link rel="prefetch" href="/assets/js/9.a7fc3cc9.js"><link rel="prefetch" href="/assets/js/90.af82565a.js"><link rel="prefetch" href="/assets/js/91.153dc863.js"><link rel="prefetch" href="/assets/js/92.ea32e690.js"><link rel="prefetch" href="/assets/js/93.4c656124.js"><link rel="prefetch" href="/assets/js/94.b2283b24.js"><link rel="prefetch" href="/assets/js/95.4ad17bba.js"><link rel="prefetch" href="/assets/js/96.4c0c4821.js"><link rel="prefetch" href="/assets/js/97.14f25834.js"><link rel="prefetch" href="/assets/js/98.92c9fd2d.js"><link rel="prefetch" href="/assets/js/99.ef340791.js">
    <link rel="stylesheet" href="/assets/css/0.styles.fefea12e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Hadoop中文网</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener noreferrer" class="nav-link external">
  下载安装
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="参考文档" class="dropdown-title"><span class="title">参考文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">通用</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">公共</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">HDFS</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">MapReduce</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">MapReduce REST APIs</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">YARN REST APIs</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">YARN Service</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Submarine</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Hadoop兼容文件系统</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Auth</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">工具</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">参考</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">配置</a></li></ul></div></div><div class="nav-item"><a href="/docs/awesome/" class="nav-link">资源教程</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="语言支持" class="dropdown-title"><span class="title">语言支持</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/language/ch.html" class="nav-link">简体中文</a></li><li class="dropdown-item"><!----> <a href="/language/en.html" class="nav-link">English</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener noreferrer" class="nav-link external">
  下载安装
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="参考文档" class="dropdown-title"><span class="title">参考文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">通用</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">公共</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">HDFS</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">MapReduce</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">MapReduce REST APIs</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">YARN REST APIs</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">YARN Service</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Submarine</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Hadoop兼容文件系统</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">Auth</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">工具</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">参考</a></li><li class="dropdown-item"><!----> <a href="/docs/hadoop-project-dist/hadoop-hdfs/.html" class="nav-link">配置</a></li></ul></div></div><div class="nav-item"><a href="/docs/awesome/" class="nav-link">资源教程</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="语言支持" class="dropdown-title"><span class="title">语言支持</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/language/ch.html" class="nav-link">简体中文</a></li><li class="dropdown-item"><!----> <a href="/language/en.html" class="nav-link">English</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>HDFS Erasure Coding</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#purpose" class="sidebar-link">Purpose</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#background" class="sidebar-link">Background</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#architecture" class="sidebar-link">Architecture</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#deployment" class="sidebar-link">Deployment</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#cluster-and-hardware-configuration" class="sidebar-link">Cluster and hardware configuration</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#configuration-keys" class="sidebar-link">Configuration keys</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#enable-intel-isa-l" class="sidebar-link">Enable Intel ISA-L</a></li><li class="sidebar-sub-header"><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#administrative-commands" class="sidebar-link">Administrative commands</a></li></ul></li><li><a href="/docs/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#limitations" class="sidebar-link">Limitations</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="hdfs-erasure-coding"><a href="#hdfs-erasure-coding" class="header-anchor">#</a> HDFS Erasure Coding</h1> <h2 id="purpose"><a href="#purpose" class="header-anchor">#</a> Purpose</h2> <p>Replication is expensive – the default 3x replication scheme in HDFS has 200% overhead in storage space and other resources (e.g., network bandwidth). However, for warm and cold datasets with relatively low I/O activities, additional block replicas are rarely accessed during normal operations, but still consume the same amount of resources as the first replica.</p> <p>Therefore, a natural improvement is to use Erasure Coding (EC) in place of replication, which provides the same level of fault-tolerance with much less storage space. In typical Erasure Coding (EC) setups, the storage overhead is no more than 50%. Replication factor of an EC file is meaningless. It is always 1 and cannot be changed via -setrep command.</p> <h2 id="background"><a href="#background" class="header-anchor">#</a> Background</h2> <p>In storage systems, the most notable usage of EC is Redundant Array of Inexpensive Disks (RAID). RAID implements EC through striping, which divides logically sequential data (such as a file) into smaller units (such as bit, byte, or block) and stores consecutive units on different disks. In the rest of this guide this unit of striping distribution is termed a striping cell (or cell). For each stripe of original data cells, a certain number of parity cells are calculated and stored – the process of which is called encoding. The error on any striping cell can be recovered through decoding calculation based on surviving data and parity cells.</p> <p>Integrating EC with HDFS can improve storage efficiency while still providing similar data durability as traditional replication-based HDFS deployments. As an example, a 3x replicated file with 6 blocks will consume 6*3 = 18 blocks of disk space. But with EC (6 data, 3 parity) deployment, it will only consume 9 blocks of disk space.</p> <h2 id="architecture"><a href="#architecture" class="header-anchor">#</a> Architecture</h2> <p>In the context of EC, striping has several critical advantages. First, it enables online EC (writing data immediately in EC format), avoiding a conversion phase and immediately saving storage space. Online EC also enhances sequential I/O performance by leveraging multiple disk spindles in parallel; this is especially desirable in clusters with high end networking. Second, it naturally distributes a small file to multiple DataNodes and eliminates the need to bundle multiple files into a single coding group. This greatly simplifies file operations such as deletion, quota reporting, and migration between federated namespaces.</p> <p>In typical HDFS clusters, small files can account for over 3/4 of total storage consumption. To better support small files, in this first phase of work HDFS supports EC with striping. In the future, HDFS will also support a contiguous EC layout. See the design doc and discussion on <a href="https://issues.apache.org/jira/browse/HDFS-7285" target="_blank" rel="noopener noreferrer">HDFS-7285<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for more information.</p> <ul><li><p>NameNode Extensions - Striped HDFS files are logically composed of block groups, each of which contains a certain number of internal blocks. To reduce NameNode memory consumption from these additional blocks, a new hierarchical block naming protocol was introduced. The ID of a block group can be inferred from the ID of any of its internal blocks. This allows management at the level of the block group rather than the block.</p></li> <li><p>Client Extensions - The client read and write paths were enhanced to work on multiple internal blocks in a block group in parallel. On the output / write path, DFSStripedOutputStream manages a set of data streamers, one for each DataNode storing an internal block in the current block group. The streamers mostly work asynchronously. A coordinator takes charge of operations on the entire block group, including ending the current block group, allocating a new block group, and so forth. On the input / read path, DFSStripedInputStream translates a requested logical byte range of data as ranges into internal blocks stored on DataNodes. It then issues read requests in parallel. Upon failures, it issues additional read requests for decoding.</p></li> <li><p>DataNode Extensions - The DataNode runs an additional ErasureCodingWorker (ECWorker) task for background recovery of failed erasure coded blocks. Failed EC blocks are detected by the NameNode, which then chooses a DataNode to do the recovery work. The recovery task is passed as a heartbeat response. This process is similar to how replicated blocks are re-replicated on failure. Reconstruction performs three key tasks:</p> <ol><li><p>Read the data from source nodes: Input data is read in parallel from source nodes using a dedicated thread pool. Based on the EC policy, it schedules the read requests to all source targets and reads only the minimum number of input blocks for reconstruction.</p></li> <li><p>Decode the data and generate the output data: New data and parity blocks are decoded from the input data. All missing data and parity blocks are decoded together.</p></li> <li><p>Transfer the generated data blocks to target nodes: Once decoding is finished, the recovered blocks are transferred to target DataNodes.</p></li></ol></li> <li><p>Erasure coding policies To accommodate heterogeneous workloads, we allow files and directories in an HDFS cluster to have different replication and erasure coding policies. The erasure coding policy encapsulates how to encode/decode a file. Each policy is defined by the following pieces of information:</p> <ol><li><p>The EC schema: This includes the numbers of data and parity blocks in an EC group (e.g., 6+3), as well as the codec algorithm (e.g., Reed-Solomon, XOR).</p></li> <li><p>The size of a striping cell. This determines the granularity of striped reads and writes, including buffer sizes and encoding work.</p></li></ol></li></ul> <p>Policies are named codec-num data blocks-num parity blocks-cell size. Currently, five built-in policies are supported: RS-3-2-1024k, RS-6-3-1024k, RS-10-4-1024k, RS-LEGACY-6-3-1024k, XOR-2-1-1024k.</p> <p>The default REPLICATION scheme is also supported. It can only be set on directory, to force the directory to adopt 3x replication scheme, instead of inheriting its ancestor’s erasure coding policy. This policy makes it possible to interleave 3x replication scheme directory with erasure coding directory.</p> <p>REPLICATION is always enabled. Out of all the EC policies, RS(6,3) is enabled by default.</p> <p>Similar to HDFS storage policies, erasure coding policies are set on a directory. When a file is created, it inherits the EC policy of its nearest ancestor directory.</p> <p>Directory-level EC policies only affect new files created within the directory. Once a file has been created, its erasure coding policy can be queried but not changed. If an erasure coded file is renamed to a directory with a different EC policy, the file retains its existing EC policy. Converting a file to a different EC policy requires rewriting its data; do this by copying the file (e.g. via distcp) rather than renaming it.</p> <p>We allow users to define their own EC policies via an XML file, which must have the following three parts:</p> <pre><code>1. layoutversion: This indicates the version of EC policy XML file format.

2. schemas: This includes all the user defined EC schemas.

3. policies: This includes all the user defined EC policies, and each policy consists of schema id and the size of a striping cell (cellsize).
</code></pre> <p>A sample EC policy XML file named user_ec_policies.xml.template is in the Hadoop conf directory, which user can reference.</p> <ul><li>Intel ISA-L Intel ISA-L stands for Intel Intelligent Storage Acceleration Library. ISA-L is an open-source collection of optimized low-level functions designed for storage applications. It includes fast block Reed-Solomon type erasure codes optimized for Intel AVX and AVX2 instruction sets. HDFS erasure coding can leverage ISA-L to accelerate encoding and decoding calculation. ISA-L supports most major operating systems, including Linux and Windows. ISA-L is not enabled by default. See the instructions below for how to enable ISA-L.</li></ul> <h2 id="deployment"><a href="#deployment" class="header-anchor">#</a> Deployment</h2> <h3 id="cluster-and-hardware-configuration"><a href="#cluster-and-hardware-configuration" class="header-anchor">#</a> Cluster and hardware configuration</h3> <p>Erasure coding places additional demands on the cluster in terms of CPU and network.</p> <p>Encoding and decoding work consumes additional CPU on both HDFS clients and DataNodes.</p> <p>Erasure coding requires a minimum of as many DataNodes in the cluster as the configured EC stripe width. For EC policy RS (6,3), this means a minimum of 9 DataNodes.</p> <p>Erasure coded files are also spread across racks for rack fault-tolerance. This means that when reading and writing striped files, most operations are off-rack. Network bisection bandwidth is thus very important.</p> <p>For rack fault-tolerance, it is also important to have enough number of racks, so that on average, each rack holds number of blocks no more than the number of EC parity blocks. A formula to calculate this would be (data blocks + parity blocks) / parity blocks, rounding up. For EC policy RS (6,3), this means minimally 3 racks (calculated by (6 + 3) / 3 = 3), and ideally 9 or more to handle planned and unplanned outages. For clusters with fewer racks than the number of the parity cells, HDFS cannot maintain rack fault-tolerance, but will still attempt to spread a striped file across multiple nodes to preserve node-level fault-tolerance. For this reason, it is recommended to setup racks with similar number of DataNodes.</p> <h3 id="configuration-keys"><a href="#configuration-keys" class="header-anchor">#</a> Configuration keys</h3> <p>By default, all built-in erasure coding policies are disabled, except the one defined in dfs.namenode.ec.system.default.policy which is enabled by default. The cluster administrator can enable set of policies through hdfs ec [-enablePolicy -policy <policyName>] command based on the size of the cluster and the desired fault-tolerance properties. For instance, for a cluster with 9 racks, a policy like RS-10-4-1024k will not preserve rack-level fault-tolerance, and RS-6-3-1024k or RS-3-2-1024k might be more appropriate. If the administrator only cares about node-level fault-tolerance, RS-10-4-1024k would still be appropriate as long as there are at least 14 DataNodes in the cluster.</policyName></p> <p>A system default EC policy can be configured via ‘dfs.namenode.ec.system.default.policy’ configuration. With this configuration, the default EC policy will be used when no policy name is passed as an argument in the ‘-setPolicy’ command.</p> <p>By default, the ‘dfs.namenode.ec.system.default.policy’ is “RS-6-3-1024k”.</p> <p>The codec implementations for Reed-Solomon and XOR can be configured with the following client and DataNode configuration keys: io.erasurecode.codec.rs.rawcoders for the default RS codec, io.erasurecode.codec.rs-legacy.rawcoders for the legacy RS codec, io.erasurecode.codec.xor.rawcoders for the XOR codec. User can also configure self-defined codec with configuration key like: io.erasurecode.codec.self-defined-codec.rawcoders. The values for these key are lists of coder names with a fall-back mechanism. These codec factories are loaded in the order specified by the configuration values, until a codec is loaded successfully. The default RS and XOR codec configuration prefers native implementation over the pure Java one. There is no RS-LEGACY native codec implementation so the default is pure Java implementation only. All these codecs have implementations in pure Java. For default RS codec, there is also a native implementation which leverages Intel ISA-L library to improve the performance of codec. For XOR codec, a native implementation which leverages Intel ISA-L library to improve the performance of codec is also supported. Please refer to section “Enable Intel ISA-L” for more detail information. The default implementation for RS Legacy is pure Java, and the default implementations for default RS and XOR are native implementations using Intel ISA-L library.</p> <p>Erasure coding background recovery work on the DataNodes can also be tuned via the following configuration parameters:</p> <ol><li>dfs.datanode.ec.reconstruction.stripedread.timeout.millis - Timeout for striped reads. Default value is 5000 ms.</li> <li>dfs.datanode.ec.reconstruction.stripedread.buffer.size - Buffer size for reader service. Default value is 64KB.</li> <li>dfs.datanode.ec.reconstruction.threads - Number of threads used by the Datanode for background reconstruction work. Default value is 8 threads.</li> <li>dfs.datanode.ec.reconstruction.xmits.weight - Relative weight of xmits used by EC background recovery task comparing to replicated block recovery. Default value is 0.5. It sets to 0 to disable calculate weights for EC recovery tasks, that is, EC task always has 1 xmits. The xmits of an erasure coding recovery task is calculated as the maximum value between the number of read streams and the number of write streams. For example, if an EC recovery task need to read from 6 nodes and write to 2 nodes, it has xmits of max(6, 2) * 0.5 = 3. Recovery task for replicated file always counts as 1 xmit. NameNode utilizes dfs.namenode.replication.max-streams minus the total xmitsInProgress on the DataNode that combines of the xmits from replicated file and EC files, to schedule recovery tasks to this DataNode.</li></ol> <h3 id="enable-intel-isa-l"><a href="#enable-intel-isa-l" class="header-anchor">#</a> Enable Intel ISA-L</h3> <p>HDFS native implementation of default RS codec leverages Intel ISA-L library to improve the encoding and decoding calculation. To enable and use Intel ISA-L, there are three steps.</p> <ol><li>Build ISA-L library. Please refer to the official site “<a href="https://github.com/01org/isa-l/" target="_blank" rel="noopener noreferrer">https://github.com/01org/isa-l/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>” for detail information.</li> <li>Build Hadoop with ISA-L support. Please refer to “Intel ISA-L build options” section in “Build instructions for Hadoop” in (BUILDING.txt) in the source code.</li> <li>Use -Dbundle.isal to copy the contents of the isal.lib directory into the final tar file. Deploy Hadoop with the tar file. Make sure ISA-L is available on HDFS clients and DataNodes.</li></ol> <p>To verify that ISA-L is correctly detected by Hadoop, run the hadoop checknative command.</p> <h3 id="administrative-commands"><a href="#administrative-commands" class="header-anchor">#</a> Administrative commands</h3> <p>HDFS provides an ec subcommand to perform administrative commands related to erasure coding.</p> <pre><code>   hdfs ec [generic options]
     [-setPolicy -path &lt;path&gt; [-policy &lt;policyName&gt;] [-replicate]]
     [-getPolicy -path &lt;path&gt;]
     [-unsetPolicy -path &lt;path&gt;]
     [-listPolicies]
     [-addPolicies -policyFile &lt;file&gt;]
     [-listCodecs]
     [-enablePolicy -policy &lt;policyName&gt;]
     [-disablePolicy -policy &lt;policyName&gt;]
     [-help [cmd ...]]
</code></pre> <p>Below are the details about each command.</p> <ul><li>[-setPolicy -path <path> [-policy <policyName>] [-replicate]]</policyName></path></li></ul> <p>Sets an erasure coding policy on a directory at the specified path.</p> <p>path: An directory in HDFS. This is a mandatory parameter. Setting a policy only affects newly created files, and does not affect existing files.</p> <p>policyName: The erasure coding policy to be used for files under this directory. This parameter can be omitted if a ‘dfs.namenode.ec.system.default.policy’ configuration is set. The EC policy of the path will be set with the default value in configuration.</p> <p>-replicate apply the default REPLICATION scheme on the directory, force the directory to adopt 3x replication scheme.</p> <p>-replicate and -policy <policyName> are optional arguments. They cannot be specified at the same time.</policyName></p> <ul><li>[-getPolicy -path <path>]</path></li></ul> <p>Get details of the erasure coding policy of a file or directory at the specified path.</p> <ul><li>[-unsetPolicy -path <path>]</path></li></ul> <p>Unset an erasure coding policy set by a previous call to setPolicy on a directory. If the directory inherits the erasure coding policy from an ancestor directory, unsetPolicy is a no-op. Unsetting the policy on a directory which doesn’t have an explicit policy set will not return an error.</p> <ul><li>[-listPolicies]</li></ul> <p>Lists all (enabled, disabled and removed) erasure coding policies registered in HDFS. Only the enabled policies are suitable for use with the setPolicy command.</p> <ul><li>[-addPolicies -policyFile <file>]</file></li></ul> <p>Add a list of user defined erasure coding policies. Please refer etc/hadoop/user_ec_policies.xml.template for the example policy file. The maximum cell size is defined in property ‘dfs.namenode.ec.policies.max.cellsize’ with the default value 4MB. Currently HDFS allows the user to add 64 policies in total, and the added policy ID is in range of 64 to 127. Adding policy will fail if there are already 64 policies added.</p> <ul><li>[-listCodecs]</li></ul> <p>Get the list of supported erasure coding codecs and coders in system. A coder is an implementation of a codec. A codec can have different implementations, thus different coders. The coders for a codec are listed in a fall back order.</p> <ul><li>[-removePolicy -policy <policyName>]</policyName></li></ul> <p>Remove an user defined erasure coding policy.</p> <ul><li>[-enablePolicy -policy <policyName>]</policyName></li></ul> <p>Enable an erasure coding policy.</p> <ul><li>[-disablePolicy -policy <policyName>]</policyName></li></ul> <p>Disable an erasure coding policy.</p> <h2 id="limitations"><a href="#limitations" class="header-anchor">#</a> Limitations</h2> <p>Certain HDFS operations, i.e., hflush, hsync, concat, setReplication, truncate and append, are not supported on erasure coded files due to substantial technical challenges.</p> <ul><li>append() and truncate() on an erasure coded file will throw IOException.</li> <li>concat() will throw IOException if files are mixed with different erasure coding policies or with replicated files.</li> <li>setReplication() is no-op on erasure coded files.</li> <li>hflush() and hsync() on DFSStripedOutputStream are no-op. Thus calling hflush() or hsync() on an erasure coded file can not guarantee data being persistent.</li></ul> <p>A client can use <a href="/docs/hadoop-project-dist/hadoop-common/filesystem/filesystem.html#interface_StreamCapabilities">StreamCapabilities</a> API to query whether a OutputStream supports hflush() and hsync(). If the client desires data persistence via hflush() and hsync(), the current remedy is creating such files as regular 3x replication files in a non-erasure-coded directory, or using FSDataOutputStreamBuilder#replicate() API to create 3x replication files in an erasure-coded directory.</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.03e63b66.js" defer></script><script src="/assets/js/2.fd33e103.js" defer></script><script src="/assets/js/62.ca728b66.js" defer></script>
  </body>
</html>
